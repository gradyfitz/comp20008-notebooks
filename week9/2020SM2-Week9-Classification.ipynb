{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K-NN  and Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "As an example dataset, we will use the [pima indian diabetes dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names), which records measurements about several hundred patients and an indication of whether or not they tested positive for diabetes (the class label).  The classification is therefore to predict whether a patient will test positive for diabetes, based on the various measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in the dataset and randomly divide it into training and test sets.   We also normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "##load in the data\n",
    "pima=pd.read_csv('pima-indians-diabetes.csv',encoding = 'ISO-8859-1')\n",
    "\n",
    "##get just the features\n",
    "data=pima[['numpregnant','plasma','blood pressure','sf-thickness','serum-insulin','BMI','pedigree-function','age']].astype(float)\n",
    "\n",
    "\n",
    "##get just the class labels\n",
    "classlabel=pima['has_diabetes']\n",
    "\n",
    "##randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,classlabel, train_size=0.66, test_size=0.34, random_state=42)\n",
    "\n",
    "#normalise the data to have 0 mean and unit variance using the library functions.  This will help for later\n",
    "#computation of distances between instances\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit a K-nearest neighbor classifier with K=5.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make predictions - for each of the test instances, we predict the class label using k-NN.   We compare our prediction with the actual class label and report the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7061068702290076\n"
     ]
    }
   ],
   "source": [
    "y_pred=knn.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Draw a scatter plot that shows varying k versus the prediction accuracy achieved by k-NN on the test set.  Based on your graph, what is the optimal k to use for this dataset?   How much better is this compared to a classifier that always make a prediction of 0 (not tested positive for diabetes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Question 1 answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a [decision tree classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).  To make the tree simple and easy to visualise, we enforce a maximum depth of 3.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\",random_state=1, max_depth=3)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we make predictions - for each of the test instances, we predict the class label.   We compare our prediction with the real class label and report the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7175572519083969\n"
     ]
    }
   ],
   "source": [
    "y_pred=dt.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Draw a graph of decision tree classification accuracy as the size of the training set is varied between 10% and 90%.   For what training size is it the most accurate?  Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Question 2 answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree visualisation\n",
    "\n",
    "The decision tree learned using 66% of the data is shown below.  The split condition is shown at the top of each node.  The true branch for the condition goes to the left and the false branch to the right.   The bottom line of each node shows the frequency of [negative, positive instances] in the node;\n",
    "\n",
    "![Decision tree](mytree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For information only) The code to generate the tree visualisation is shown below.  It is commented out, since it is tricky to get the graphviz library installed and this is not currently working on the University lab machines.   If you are using a Mac on your home macine - one can use the \"brew\" package manager and then do \"brew install graphviz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import export_graphviz\n",
    "#import graphviz\n",
    "\n",
    "\n",
    "#export_graphviz(dt, out_file=\"mytree.dot\",feature_names = data.columns,filled=True,rounded=True)\n",
    "#with open(\"./mytree.dot\") as f:\n",
    "#    dot_graph = f.read()\n",
    "    \n",
    "#graphviz.Source(dot_graph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
